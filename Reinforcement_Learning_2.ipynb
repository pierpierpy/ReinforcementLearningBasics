{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOoCKINJNq13ngkq4z0pzAe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":65,"metadata":{"id":"2H-d0rB3lyBM","executionInfo":{"status":"ok","timestamp":1759485417467,"user_tz":-120,"elapsed":2,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}}},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","from IPython.display import display, clear_output"]},{"cell_type":"markdown","source":["# Value function"],"metadata":{"id":"fzBZOPOThWjw"}},{"cell_type":"code","source":["\n","# Setup: 4 states in a line: 0 -> 1 -> 2 -> 3 (goal)\n","states = 4\n","actions = 2\n","num_rewards = 2\n","γ = 0.9\n","\n","possible_rewards = torch.tensor([0.0, 1.0])\n","\n","# Joint distribution: p(s', r | s, a)\n","joint_dist = torch.zeros(states, actions, states, num_rewards)\n","\n","# State 0\n","joint_dist[0, 0, 0, 0] = 1.0  # left -> stay, reward 0\n","joint_dist[0, 1, 1, 0] = 1.0  # right -> 1, reward 0\n","\n","# State 1\n","joint_dist[1, 0, 0, 0] = 1.0  # left -> 0, reward 0\n","joint_dist[1, 1, 2, 0] = 1.0  # right -> 2, reward 0\n","\n","# State 2\n","joint_dist[2, 0, 1, 0] = 1.0  # left -> 1, reward 0\n","joint_dist[2, 1, 3, 0] = 0.3  # right -> 3, reward 0\n","joint_dist[2, 1, 3, 1] = 0.7  # right -> 3, reward 1 (stochastic!)\n","\n","# State 3 (goal) --> # any action just stays at 3 --> absorbing state!\n","joint_dist[3, 0, 3, 0] = 1.0\n","joint_dist[3, 1, 3, 0] = 1.0\n","\n","\n","# uniform distribution for policy\n","policy = torch.ones(states, actions) / actions\n","\n","v = torch.zeros(states)\n"],"metadata":{"id":"1g5rNi7AFPi6","executionInfo":{"status":"ok","timestamp":1759485418058,"user_tz":-120,"elapsed":3,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# Value function\n","# v_new = r + γ * P @ v\n","\n","def state_value(s):\n","    # First sum: r[s] --> sum over all actions and all future states and rewards\n","    immediate_reward = 0.0\n","    for a in range(actions):\n","        for s_prime in range(states):\n","            for r_idx in range(num_rewards):\n","                prob = joint_dist[s, a, s_prime, r_idx]\n","                reward = possible_rewards[r_idx]\n","                immediate_reward += policy[s, a] * prob * reward\n","\n","    # Second sum: (P @ v)[s] --> sum over all actions and all future states and rewards\n","    future_value = 0.0\n","    for a in range(actions):\n","        for s_prime in range(states):\n","            for r_idx in range(num_rewards):\n","                prob = joint_dist[s, a, s_prime, r_idx]\n","                Ps_s_prime = policy[s, a] * prob\n","                future_value += Ps_s_prime * v[s_prime]\n","\n","    s_value = immediate_reward + γ * future_value\n","    return s_value\n","\n","\n","# # this is doing the same thing, but it is calculating everything all at once using the \"unified formula\" 3.14\n","# def state_value_compact(s):\n","#     total = 0.0\n","#     for a in range(actions):\n","#         for s_prime in range(states):\n","#             for r_idx in range(num_rewards):\n","#                 prob_ = joint_dist[s, a, s_prime, r_idx]\n","#                 reward_ = possible_rewards[r_idx]\n","#                 policy_ = policy[s, a]\n","#                 value_function_ = v[s_prime]\n","#\n","#                 total += policy_ * prob_ * (reward_ + γ *value_function_ )\n","#     return total\n"],"metadata":{"id":"zHQwTp6v0vZh","executionInfo":{"status":"ok","timestamp":1759486137438,"user_tz":-120,"elapsed":3,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["\n","for iteration in range(100):\n","    v_new = torch.zeros(states)\n","\n","    for s in range(states):\n","        v_new[s] = state_value(s)\n","\n","    v = v_new\n","\n","\n","v\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5RppIFk90wwu","executionInfo":{"status":"ok","timestamp":1759485421188,"user_tz":-120,"elapsed":278,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"107331a6-d10d-4ccc-fcf2-c3a26d9fe55d"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.3002, 0.3669, 0.5151, 0.0000])"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["P = torch.zeros(states, states)\n","r = torch.zeros(states)\n","\n","\n","for s in range(states):\n","    for a in range(actions):\n","        for s_prime in range(states):\n","            for r_idx in range(num_rewards):\n","                prob = joint_dist[s, a, s_prime, r_idx]\n","                Ps_s_prime = policy[s, a] * prob\n","                P[s, s_prime] += Ps_s_prime\n","\n","\n","for s in range(states):\n","    for a in range(actions):\n","        for s_prime in range(states):\n","            for r_idx in range(num_rewards):\n","                prob = joint_dist[s, a, s_prime, r_idx]\n","                reward_val = possible_rewards[r_idx]\n","                r[s] += policy[s, a] * prob * reward_val\n","\n","# Solve\n","I = torch.eye(states)\n","A = I - γ * P\n","v_direct = torch.linalg.solve(A, r)\n","\n","v_direct\n","\n","\n","\n","\n","# P = torch.zeros(states, states)\n","# r = torch.zeros(states)\n","#\n","# for s in range(states):\n","#     for a in range(actions):\n","#         for s_prime in range(states):\n","#             for r_idx in range(num_rewards):\n","#                 prob = joint_dist[s, a, s_prime, r_idx]\n","#                 reward_val = possible_rewards[r_idx]\n","#                 P[s, s_prime] += policy[s, a] * prob\n","#                 r[s] += policy[s, a] * prob * reward_val\n","#\n","# I = torch.eye(states)\n","# v_direct = torch.linalg.solve(I - γ * P, r)\n","# v_direct # --> calculated the same way but all in one\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3BbvGKf1OQM","executionInfo":{"status":"ok","timestamp":1759485423373,"user_tz":-120,"elapsed":19,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"1b8bee1e-d487-46a0-aa57-ff8f7b0efca9"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.3002, 0.3669, 0.5151, 0.0000])"]},"metadata":{},"execution_count":69}]}]}